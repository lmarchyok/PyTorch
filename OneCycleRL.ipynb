{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1uSdvvvhb9uoxFd5SoI_ki5aAt_SRx-kW",
      "authorship_tag": "ABX9TyM1tRmq1aQZQDWyjPclCV0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmarchyok/PyTorch/blob/main/OneCycleRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HtyR2597VHj"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import math\n",
        "\n",
        "\n",
        "class OneCycleLR(StepLR):\n",
        "    def __init__(self,\n",
        "                 optimizer,\n",
        "                 max_lr,\n",
        "                 total_steps=None,\n",
        "                 epochs=None,\n",
        "                 steps_per_epoch=None,\n",
        "                 pct_start=0.3,\n",
        "                 anneal_strategy='cos',\n",
        "                 cycle_momentum=True,\n",
        "                 base_momentum=0.85,\n",
        "                 max_momentum=0.95,\n",
        "                 div_factor=25.,\n",
        "                 final_div_factor=1e4,\n",
        "                 three_phase=False,\n",
        "                 last_epoch=-1,\n",
        "                 verbose=False):\n",
        "\n",
        "        self.last_epoch = last_epoch\n",
        "\n",
        "        # Validate optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Validate total_steps\n",
        "        if total_steps is None and epochs is None and steps_per_epoch is None:\n",
        "            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n",
        "        elif total_steps is not None:\n",
        "            if total_steps <= 0 or not isinstance(total_steps, int):\n",
        "                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n",
        "            self.total_steps = total_steps\n",
        "        else:\n",
        "            if epochs <= 0 or not isinstance(epochs, int):\n",
        "                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n",
        "            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n",
        "                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n",
        "            self.total_steps = epochs * steps_per_epoch\n",
        "\n",
        "        if three_phase:\n",
        "            self._schedule_phases = [\n",
        "                {\n",
        "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'max_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'base_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': float(2 * pct_start * self.total_steps) - 2,\n",
        "                    'start_lr': 'max_lr',\n",
        "                    'end_lr': 'initial_lr',\n",
        "                    'start_momentum': 'base_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': self.total_steps - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'min_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "            ]\n",
        "        else:\n",
        "            self._schedule_phases = [\n",
        "                {\n",
        "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'max_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'base_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': self.total_steps - 1,\n",
        "                    'start_lr': 'max_lr',\n",
        "                    'end_lr': 'min_lr',\n",
        "                    'start_momentum': 'base_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "            ]\n",
        "\n",
        "        # Validate pct_start\n",
        "        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n",
        "            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n",
        "\n",
        "        # Validate anneal_strategy\n",
        "        if anneal_strategy not in ['cos', 'linear']:\n",
        "            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n",
        "        elif anneal_strategy == 'cos':\n",
        "            self.anneal_func = self._annealing_cos\n",
        "        elif anneal_strategy == 'linear':\n",
        "            self.anneal_func = self._annealing_linear\n",
        "\n",
        "        # Initialize learning rate variables\n",
        "        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n",
        "        if last_epoch == -1:\n",
        "            for idx, group in enumerate(self.optimizer.param_groups):\n",
        "                group['initial_lr'] = max_lrs[idx] / div_factor\n",
        "                group['max_lr'] = max_lrs[idx]\n",
        "                group['min_lr'] = group['initial_lr'] / final_div_factor\n",
        "\n",
        "        # Initialize momentum variables\n",
        "        self.cycle_momentum = cycle_momentum\n",
        "        if self.cycle_momentum:\n",
        "            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n",
        "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
        "            self.use_beta1 = 'betas' in self.optimizer.defaults\n",
        "            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
        "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
        "            if last_epoch == -1:\n",
        "                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n",
        "                    if self.use_beta1:\n",
        "                        _, beta2 = group['betas']\n",
        "                        group['betas'] = (m_momentum, beta2)\n",
        "                    else:\n",
        "                        group['momentum'] = m_momentum\n",
        "                    group['max_momentum'] = m_momentum\n",
        "                    group['base_momentum'] = b_momentum\n",
        "\n",
        "        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)\n",
        "\n",
        "    def _format_param(self, name, optimizer, param):\n",
        "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
        "        if isinstance(param, (list, tuple)):\n",
        "            if len(param) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
        "                    len(optimizer.param_groups), name, len(param)))\n",
        "            return param\n",
        "        else:\n",
        "            return [param] * len(optimizer.param_groups)\n",
        "\n",
        "    def _annealing_cos(self, start, end, pct):\n",
        "        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
        "        cos_out = math.cos(math.pi * pct) + 1\n",
        "        return end + (start - end) / 2.0 * cos_out\n",
        "\n",
        "    def _annealing_linear(self, start, end, pct):\n",
        "        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
        "        return (end - start) * pct + start\n",
        "\n",
        "    def get_lr(self):\n",
        "        pass\n",
        "        # if not self._get_lr_called_within_step:\n",
        "          #  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "           #               \"please use `get_last_lr()`.\", UserWarning)\n",
        "\n",
        "        lrs = []\n",
        "        step_num = self.last_epoch\n",
        "\n",
        "        if step_num > self.total_steps:\n",
        "            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n",
        "                             .format(step_num + 1, self.total_steps))\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            start_step = 0\n",
        "            for i, phase in enumerate(self._schedule_phases):\n",
        "                end_step = phase['end_step']\n",
        "                if step_num <= end_step or i == len(self._schedule_phases) - 1:\n",
        "                    pct = (step_num - start_step) / (end_step - start_step)\n",
        "                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)\n",
        "                    if self.cycle_momentum:\n",
        "                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)\n",
        "                    break\n",
        "                start_step = phase['end_step']\n",
        "\n",
        "            lrs.append(computed_lr)\n",
        "            if self.cycle_momentum:\n",
        "                if self.use_beta1:\n",
        "                    _, beta2 = group['betas']\n",
        "                    group['betas'] = (computed_momentum, beta2)\n",
        "                else:\n",
        "                    group['momentum'] = computed_momentum\n",
        "\n",
        "        return lrs"
      ]
    }
  ]
}