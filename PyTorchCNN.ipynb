{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOh1BWcspdwzB/BnVCcHDme",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmarchyok/PyTorch/blob/main/PyTorchCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYTA89pjICQI",
        "outputId": "3b6eae2e-fecb-42da-90e8-16c54e0f2b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: ./cifar10.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [0], last_lr: 0.00393, train_loss: 1.4905, val_loss: 1.7335, val_acc: 0.5140\n",
            "Epoch [1], last_lr: 0.00935, train_loss: 1.1148, val_loss: 1.0624, val_acc: 0.6502\n",
            "Epoch [2], last_lr: 0.00972, train_loss: 0.8527, val_loss: 0.9692, val_acc: 0.6768\n",
            "Epoch [3], last_lr: 0.00812, train_loss: 0.6245, val_loss: 0.6533, val_acc: 0.7850\n",
            "Epoch [4], last_lr: 0.00556, train_loss: 0.5146, val_loss: 0.4653, val_acc: 0.8361\n",
            "Epoch [5], last_lr: 0.00283, train_loss: 0.4039, val_loss: 0.3728, val_acc: 0.8735\n",
            "Epoch [6], last_lr: 0.00077, train_loss: 0.2984, val_loss: 0.3016, val_acc: 0.8976\n",
            "Epoch [7], last_lr: 0.00000, train_loss: 0.2169, val_loss: 0.2822, val_acc: 0.9035\n",
            "[{'val_loss': 1.7334861755371094, 'val_acc': 0.5140476226806641, 'train_loss': 1.4904868602752686, 'lrs': [0.0003999999999999993, 0.0004002022776625272, 0.00040080909360167005, 0.0004018203966735523, 0.00040323610164316043, 0.0004050560891915439, 0.00040728020592585394, 0.0004099082643922898, 0.0004129400430918778, 0.0004163752864991578, 0.0004202137050837128, 0.00042445497533456604, 0.0004290987397874528, 0.00043414460705495234, 0.00043959215185946157, 0.0004454409150690553, 0.0004516904037361689, 0.0004583400911391533, 0.00046538941682666354, 0.0004728377866649019, 0.0004806845728876803, 0.0004889291141493453, 0.0004975707155805073, 0.0005066086488466071, 0.0005160421522093072, 0.0005258704305906858, 0.0005360926556402506, 0.0005467079658047595, 0.0005577154664008245, 0.0005691142296903231, 0.0005809032949585929, 0.0005930816685953983, 0.0006056483241786755, 0.0006186022025610419, 0.0006319422119590631, 0.0006456672280452744, 0.0006597760940429374, 0.0006742676208235376, 0.0006891405870070068, 0.000704393739064663, 0.0007200257914248623, 0.000736035426581353, 0.0007524212952043068, 0.0007691820162540591, 0.0007863161770974934, 0.000803822333627106, 0.0008216990103827246, 0.0008399447006758511, 0.0008585578667166596, 0.0008775369397436023, 0.0008968803201556234, 0.0009165863776469853, 0.0009366534513446656, 0.0009570798499483491, 0.000977863851872967, 0.0009990037053938007, 0.0010204976287941162, 0.00104234381051534, 0.0010645404093097294, 0.001087085554395571, 0.0011099773456148421, 0.0011332138535933688, 0.0011567931199034341, 0.0011807131572288427, 0.0012049719495324102, 0.001229567452225886, 0.0012544975923422744, 0.0012797602687105447, 0.001305353352132728, 0.0013312746855633686, 0.0013575220842913308, 0.001384093336123918, 0.001410986201573334, 0.0014381984140454298, 0.0014657276800307349, 0.0014935716792977578, 0.0015217280650885456, 0.001550194464316472, 0.0015789684777662485, 0.0016080476802961315, 0.0016374296210423245, 0.0016671118236255347, 0.0016970917863596974, 0.0017273669824628222, 0.0017579348602699465, 0.0017887928434482097, 0.0018199383312139834, 0.001851368698552073, 0.0018830812964369635, 0.0019150734520560868, 0.0019473424690350776, 0.001979885627665056, 0.002012700185131831, 0.0020457833757470713, 0.002079132411181423, 0.0021127444806994904, 0.0021466167513967543, 0.0021807463684383223, 0.0022151304552995446, 0.002249766114008454, 0.0022846504253900167, 0.0023197804493121643, 0.002355153224933594, 0.0023907657709533207, 0.002426615085861939, 0.0024626981481946078, 0.002499011916785701, 0.0025355533310251217, 0.002572319311116262, 0.0026093067583355762, 0.002646512555293745, 0.0026839335661984157, 0.002721566637118503, 0.0027594085962499935, 0.002797456254183291, 0.0028357064041720156, 0.002874155822403279, 0.002912801268269394, 0.002951639484641007, 0.0029906671981416045, 0.003029881119423413, 0.003069277943444626, 0.003108854349747962, 0.003148607002740521, 0.003188532551974918, 0.003228627632431663, 0.003268888864802778, 0.003309312855776604, 0.003349896198323806, 0.0033906354719845247, 0.003431527243156654, 0.0034725680653852394, 0.0035137544796529545, 0.0035550830146716308, 0.003596550187174823, 0.0036381525022113996, 0.0036798864534400904, 0.0037217485234250216, 0.0037637351839321645, 0.0038058428962267094, 0.0038480681113713154, 0.003890407270525225, 0.00393285680524421]}, {'val_loss': 1.062441110610962, 'val_acc': 0.6501904726028442, 'train_loss': 1.1148024797439575, 'lrs': [0.00397541313778133, 0.004018072681388472, 0.0040608318406186485, 0.004103687011629035, 0.00414663458248471, 0.0041896709334630685, 0.0042327924373589095, 0.004275995459790148, 0.004319276359504119, 0.004362631488684477, 0.004406057193258646, 0.004449549813205786, 0.0044931056828652685, 0.004536721131245638, 0.004580392482333997, 0.004624116055405837, 0.004667888165335261, 0.0047117051229055695, 0.004755563235120194, 0.0047994588055139615, 0.004843388134464631, 0.004887347519504715, 0.00493133325563352, 0.0049753416356294305, 0.005019368950362348, 0.005063411489106304, 0.005107465539852224, 0.005151527389620771, 0.005195593324775287, 0.005239659631334787, 0.005283722595286985, 0.005327778502901319, 0.005371823641041949, 0.005415854297480714, 0.005459866761210004, 0.005503857322755533, 0.005547822274488984, 0.005591757910940491, 0.005635660529110959, 0.00567952642878414, 0.0057233519128385155, 0.005767133287558888, 0.005810866862947697, 0.005854548953036027, 0.0058981758761942615, 0.005941743955442384, 0.00598524951875988, 0.006028688899395228, 0.006072058436174937, 0.006115354473812123, 0.006158573363214582, 0.0062017114617923435, 0.006244765133764682, 0.0062877307504665405, 0.006330604690654371, 0.0063733833408113425, 0.006416063095451885, 0.0064586403574255795, 0.0065011115382203315, 0.0065434730582648185, 0.00658572134723018, 0.006627852844330946, 0.006669863998625137, 0.006711751269313545, 0.00675351112603817, 0.006795140049179759, 0.006836634530154451, 0.006877991071709489, 0.006919206188217975, 0.006960276405972646, 0.007001198263478648, 0.0070419683117452815, 0.007082583114576686, 0.0071230392488614526, 0.007163333304861134, 0.007203461886497618, 0.007243421611639369, 0.007283209112386468, 0.007322821035354478, 0.007362254041957076, 0.007401504808687424, 0.007440570027398298, 0.007479446405580889, 0.0075181306666423225, 0.007556619550181807, 0.0075949098122654254, 0.007632998225699548, 0.007670881580302826, 0.007708556683176751, 0.007746020358974764, 0.007783269450169876, 0.007820300817320797, 0.007857111339336531, 0.007893697913739435, 0.007930057456926695, 0.007966186904430222, 0.008002083211174942, 0.008037743351735425, 0.008073164320590893, 0.008108343132378515, 0.008143276822145036, 0.008177962445596657, 0.008212397079347202, 0.008246577821164489, 0.008280501790214953, 0.008314166127306444, 0.008347567995129205, 0.008380704578495006, 0.008413573084574423, 0.008446170743132208, 0.008478494806760791, 0.008510542551111826, 0.008542311275125803, 0.008573798301259707, 0.008605000975712688, 0.008635916668649725, 0.00866654277442327, 0.008696876711792874, 0.008726915924142724, 0.008756657879697129, 0.008786100071733902, 0.008815240018795635, 0.008844075264898835, 0.00887260337974093, 0.008900821958905087, 0.008928728624062878, 0.008956321023174722, 0.00898359683068812, 0.00901055374773366, 0.009037189502318773, 0.009063501849519217, 0.00908948857166829, 0.009115147478543736, 0.009140476407552344, 0.009165473223912217, 0.009190135820832696, 0.009214462119691923, 0.009238450070212033, 0.009262097650631962, 0.009285402867877837, 0.009308363757730965, 0.009330978384993372, 0.009353244843650918]}, {'val_loss': 0.9691572189331055, 'val_acc': 0.6767618656158447, 'train_loss': 0.8527085185050964, 'lrs': [0.00937516125703393, 0.009396725777975382, 0.009417936588966568, 0.009438791902310294, 0.009459289960271544, 0.00947942903522563, 0.009499207429803803, 0.0095186234770363, 0.009537675540492849, 0.009556362014420594, 0.009574681323879421, 0.009592631924874709, 0.00961021230448745, 0.009627420981001776, 0.009644256504029825, 0.009660717454633996, 0.009676802445446536, 0.009692510120786462, 0.009707839156773845, 0.009722788261441358, 0.009737356174843193, 0.009751541669161234, 0.009765343548808553, 0.009778760650530169, 0.009791791843501093, 0.009804436029421635, 0.009816692142609973, 0.00982855915009197, 0.00984003605168824, 0.009851121880098433, 0.009861815700982778, 0.00987211661304082, 0.009882023748087386, 0.00989153627112576, 0.009900653380418057, 0.009909374307552793, 0.009917698317509655, 0.009925624708721444, 0.009933152813133207, 0.009940281996258543, 0.009947011657233077, 0.009953341228865105, 0.009959270177683393, 0.00996479800398215, 0.009969924241863132, 0.009974648459274916, 0.009978970258049308, 0.009982889273934916, 0.009986405176627834, 0.009989517669799485, 0.009992226491121602, 0.009994531412288333, 0.009996432239035484, 0.009997928811156891, 0.009999021002517927, 0.009999708721066125, 0.009999991908838943, 0.009999975375381807, 0.009999875338286097, 0.009999698351212663, 0.009999444416885415, 0.009999113539212512, 0.00999870572328631, 0.009998220975383275, 0.00999765930296389, 0.009997020714672542, 0.009996305220337384, 0.009995512830970193, 0.00999464355876619, 0.009993697417103854, 0.009992674420544726, 0.009991574584833175, 0.009990397926896152, 0.009989144464842944, 0.009987814217964887, 0.009986407206735062, 0.009984923452807997, 0.009983362979019316, 0.0099817258093854, 0.009980011969103012, 0.009978221484548907, 0.009976354383279432, 0.0099744106940301, 0.009972390446715145, 0.009970293672427064, 0.009968120403436137, 0.00996587067318993, 0.009963544516312787, 0.009961141968605286, 0.009958663067043697, 0.009956107849779404, 0.00995347635613833, 0.00995076862662032, 0.009947984702898528, 0.009945124627818764, 0.009942188445398846, 0.009939176200827918, 0.009936087940465747, 0.009932923711842023, 0.009929683563655624, 0.009926367545773852, 0.009922975709231688, 0.00991950810623099, 0.0099159647901397, 0.00991234581549101, 0.009908651237982537, 0.00990488111447546, 0.009901035502993639, 0.009897114462722728, 0.00989311805400927, 0.009889046338359755, 0.009884899378439681, 0.009880677238072589, 0.009876379982239083, 0.009872007677075821, 0.009867560389874509, 0.009863038189080858, 0.00985844114429353, 0.00985376932626307, 0.009849022806890822, 0.009844201659227806, 0.009839305957473614, 0.00983433577697525, 0.009829291194225986, 0.009824172286864174, 0.009818979133672053, 0.00981371181457454, 0.009808370410638001, 0.009802955004068994, 0.009797465678213015, 0.009791902517553209, 0.009786265607709068, 0.009780555035435123, 0.009774770888619597, 0.00976891325628306, 0.009762982228577055, 0.009756977896782712, 0.009750900353309347, 0.009744749691693033, 0.009738526006595161, 0.009732229393800989, 0.009725859950218164, 0.009719417773875232]}, {'val_loss': 0.6533234119415283, 'val_acc': 0.7849524021148682, 'train_loss': 0.6245483160018921, 'lrs': [0.009712902963920125, 0.00970631562061864, 0.009699655845352895, 0.00969292374061977, 0.00968611941002932, 0.009679242958303191, 0.009672294491273013, 0.009665274115878753, 0.009658181940167081, 0.00965101807328971, 0.00964378262550171, 0.00963647570815981, 0.009629097433720691, 0.009621647915739245, 0.009614127268866846, 0.009606535608849555, 0.009598873052526373, 0.009591139717827418, 0.009583335723772117, 0.009575461190467384, 0.009567516239105754, 0.00955950099196353, 0.009551415572398901, 0.009543260104850041, 0.009535034714833185, 0.009526739528940717, 0.0095183746748392, 0.009509940281267427, 0.009501436478034435, 0.009492863396017502, 0.009484221167160144, 0.009475509924470066, 0.00946672980201714, 0.009457880934931316, 0.009448963459400562, 0.009439977512668757, 0.009430923233033581, 0.009421800759844392, 0.00941261023350007, 0.00940335179544687, 0.009394025588176234, 0.009384631755222602, 0.009375170441161203, 0.009365641791605837, 0.009356045953206616, 0.009346383073647725, 0.00933665330164514, 0.00932685678694434, 0.009316993680318006, 0.009307064133563695, 0.00929706829950151, 0.009287006331971746, 0.009276878385832516, 0.009266684616957377, 0.009256425182232925, 0.009246100239556382, 0.009235709947833171, 0.009225254466974456, 0.009214733957894702, 0.00920414858250918, 0.009193498503731483, 0.009182783885471016, 0.009172004892630481, 0.009161161691103328, 0.009150254447771207, 0.009139283330501404, 0.00912824850814425, 0.009117150150530521, 0.009105988428468837, 0.00909476351374302, 0.009083475579109457, 0.009072124798294437, 0.00906071134599148, 0.009049235397858648, 0.009037697130515844, 0.009026096721542086, 0.009014434349472782, 0.009002710193796984, 0.00899092443495461, 0.008979077254333686, 0.008967168834267547, 0.008955199358032023, 0.008943169009842634, 0.008931077974851738, 0.008918926439145696, 0.008906714589742, 0.008894442614586393, 0.008882110702549984, 0.008869719043426335, 0.008857267827928544, 0.008844757247686304, 0.00883218749524296, 0.008819558764052545, 0.008806871248476799, 0.008794125143782179, 0.008781320646136853, 0.008768457952607688, 0.008755537261157204, 0.008742558770640542, 0.00872952268080239, 0.008716429192273917, 0.008703278506569686, 0.008690070826084544, 0.008676806354090518, 0.008663485294733677, 0.008650107853030999, 0.008636674234867208, 0.00862318464699161, 0.008609639297014908, 0.008596038393406011, 0.00858238214548882, 0.008568670763439012, 0.008554904458280797, 0.008541083441883686, 0.008527207926959206, 0.008513278127057655, 0.008499294256564795, 0.008485256530698551, 0.008471165165505716, 0.008457020377858613, 0.008442822385451758, 0.008428571406798513, 0.008414267661227721, 0.008399911368880335, 0.00838550275070602, 0.00837104202845976, 0.008356529424698448, 0.008341965162777453, 0.008327349466847185, 0.008312682561849647, 0.008297964673514974, 0.008283196028357956, 0.008268376853674552, 0.00825350737753839, 0.008238587828797266, 0.008223618437069612, 0.008208599432740963, 0.00819353104696042, 0.008178413511637079, 0.008163247059436475, 0.008148031923776992, 0.008132768338826278, 0.00811745653949763]}, {'val_loss': 0.46527615189552307, 'val_acc': 0.8361428380012512, 'train_loss': 0.5146297812461853, 'lrs': [0.008102096761446392, 0.008086689241066319, 0.008071234215485938, 0.008055731922564912, 0.008040182600890358, 0.008024586489773194, 0.008008943829244443, 0.007993254860051547, 0.007977519823654659, 0.007961738962222924, 0.00794591251863076, 0.007930040736454105, 0.007914123859966685, 0.007898162134136247, 0.007882155804620788, 0.007866105117764775, 0.007850010320595355, 0.007833871660818548, 0.007817689386815445, 0.007801463747638379, 0.007785194993007086, 0.00776888337330487, 0.007752529139574752, 0.007736132543515599, 0.00771969383747825, 0.007703213274461638, 0.007686691108108894, 0.0076701275927034416, 0.0076535229831650835, 0.007636877535046081, 0.007620191504527218, 0.007603465148413859, 0.007586698724131999, 0.0075698924897243, 0.007553046703846117, 0.0075361616257615185, 0.007519237515339303, 0.00750227463304899, 0.007485273239956815, 0.00746823359772171, 0.007451155968591282, 0.007434040615397775, 0.007416887801554016, 0.007399697791049373, 0.007382470848445684, 0.00736520723887319, 0.007347907228026453, 0.0073305710821602674, 0.007313199068085558, 0.00729579145316528, 0.007278348505310304, 0.007260870492975282, 0.007243357685154533, 0.007225810351377884, 0.007208228761706544, 0.0071906131867289225, 0.007172963897556488, 0.00715528116581958, 0.007137565263663231, 0.00711981646374299, 0.0071020350392207105, 0.007084221263760354, 0.007066375411523777, 0.007048497757166512, 0.007030588575833542, 0.007012648143155062, 0.006994676735242241, 0.0069766746286829655, 0.006958642100537594, 0.006940579428334683, 0.006922486890066721, 0.006904364764185847, 0.006886213329599566, 0.00686803286566646, 0.006849823652191882, 0.0068315859694236545, 0.006813320098047755, 0.006795026319183996, 0.006776704914381699, 0.006758356165615361, 0.006739980355280313, 0.006721577766188377, 0.006703148681563511, 0.006684693385037453, 0.006666212160645351, 0.006647705292821393, 0.006629173066394438, 0.006610615766583617, 0.006592033678993956, 0.006573427089611972, 0.00655479628480128, 0.006536141551298181, 0.0065174631762072435, 0.006498761446996895, 0.006480036651494991, 0.0064612890778843945, 0.006442519014698521, 0.006423726750816919, 0.006404912575460817, 0.006386076778188662, 0.006367219648891683, 0.006348341477789409, 0.0063294425554252165, 0.006310523172661852, 0.006291583620676955, 0.0062726241909585815, 0.006253645175300712, 0.006234646865798763, 0.006215629554845093, 0.006196593535124502, 0.006177539099609726, 0.006158466541556924, 0.006139376154501173, 0.006120268232251946, 0.006101143068888592, 0.006082000958755803, 0.006062842196459092, 0.006043667076860257, 0.0060244758950728445, 0.006005268946457599, 0.005986046526617926, 0.005966808931395344, 0.005947556456864921, 0.005928289399330727, 0.005909008055321272, 0.005889712721584941, 0.005870403695085423, 0.005851081272997153, 0.005831745752700718, 0.0058123974317783045, 0.005793036608009096, 0.005773663579364709, 0.005754278644004594, 0.005734882100271451, 0.005715474246686638, 0.005696055381945576, 0.005676625804913156, 0.005657185814619134, 0.005637735710253528, 0.005618275791162019, 0.005598806356841341, 0.005579327706934672, 0.005559840141227017]}, {'val_loss': 0.37282297015190125, 'val_acc': 0.8734762072563171, 'train_loss': 0.4038848876953125, 'lrs': [0.005540343959640605, 0.005520839462230266, 0.005501326949178814, 0.005481806720792423, 0.005462279077496014, 0.005442744319828628, 0.005423202748438795, 0.0054036546640799125, 0.005384100367605617, 0.005364540159965153, 0.005344974342198735, 0.005325403215432927, 0.005305827080875997, 0.0052862462398132835, 0.0052666609936025625, 0.0052470716436694076, 0.005227478491502551, 0.0052078818386492405, 0.005188281986710602, 0.005168679237336996, 0.005149073892223377, 0.005129466253104654, 0.005109856621751029, 0.005090245299963378, 0.005070632589568584, 0.005051018792414912, 0.005031404210367341, 0.00501178914530294, 0.004992173899106203, 0.0049725587736644174, 0.0049529440708630116, 0.004933330092580907, 0.004913717140685874, 0.004894105517029894, 0.004874495523444495, 0.0048548874617361255, 0.004835281633681497, 0.004815678341022948, 0.004796077885463794, 0.004776480568663685, 0.004756886692233964, 0.004737296557733032, 0.004717710466661691, 0.004698128720458518, 0.004678551620495218, 0.004658979468071992, 0.0046394125644128896, 0.004619851210661186, 0.00460029570787474, 0.00458074635702136, 0.004561203458974171, 0.004541667314506989, 0.004522138224289692, 0.004502616488883587, 0.004483102408736786, 0.004463596284179583, 0.004444098415419837, 0.0044246091025383445, 0.004405128645484219, 0.004385657344070285, 0.00436619549796845, 0.00434674340670511, 0.004327301369656519, 0.004307869686044205, 0.004288448654930339, 0.004269038575213153, 0.004249639745622331, 0.004230252464714411, 0.0042108770308681934, 0.0041915137422801445, 0.004172162896959811, 0.004152824792725236, 0.004133499727198368, 0.004114187997800483, 0.00409488990174761, 0.004075605736045954, 0.0040563357974873265, 0.004037080382644575, 0.004017839787867024, 0.003998614309275909, 0.003979404242759821, 0.003960209883970151, 0.003941031528316542, 0.003921869470962342, 0.0039027240068200612, 0.0038835954305468305, 0.003864484036539875, 0.0038453901189319712, 0.0038263139715869273, 0.0038072558880950617, 0.003788216161768681, 0.003769195085637564, 0.0037501929524444595, 0.0037312100546405724, 0.003712246684381071, 0.0036933031335205807, 0.0036743796936087, 0.003655476655885513, 0.0036365943112771013, 0.003617732950391074, 0.0035988928635120854, 0.0035800743405973827, 0.003561277671272328, 0.0035425031448259475, 0.003523751050206481, 0.0035050216760169283, 0.003486315310510617, 0.0034676322415867584, 0.0034489727567860165, 0.0034303371432860916, 0.003411725687897288, 0.003393138677058111, 0.003374576396830849, 0.003356039132897178, 0.003337527170553761, 0.0033190407947078604, 0.0033005802898729473, 0.0032821459401643324, 0.0032637380292947823, 0.0032453568405701595, 0.003227002656885063, 0.0032086757607184694, 0.0031903764341293895, 0.003172104958752525, 0.00315386161579394, 0.003135646686026723, 0.003117460449786674, 0.003099303186967983, 0.00308117517701893, 0.0030630766989375815, 0.0030450080312674917, 0.0030269694520934187, 0.003008961239037053, 0.0029909836692527283, 0.0029730370194231705, 0.002955121565755231, 0.00293723758397564, 0.0029193853493267594, 0.0029015651365623505, 0.0028837772199433458, 0.0028660218732336215, 0.00284829936969579, 0.0028306099820869924]}, {'val_loss': 0.30155041813850403, 'val_acc': 0.8976190686225891, 'train_loss': 0.2983548939228058, 'lrs': [0.002812953982654704, 0.002795331643132539, 0.0027777432347360718, 0.0027601890281586587, 0.0027426692935672782, 0.0027251843005983716, 0.0027077343183536835, 0.002690319615396138, 0.0026729404597456846, 0.002655597118875195, 0.0026382898597063307, 0.0026210189486054365, 0.0026037846513794526, 0.0025865872332718042, 0.0025694269589583403, 0.0025523040925432496, 0.002535218897554988, 0.0025181716369422464, 0.0025011625730698736, 0.0024841919677148637, 0.002467260082062316, 0.0024503671767014098, 0.0024335135116214084, 0.0024166993462076463, 0.0023999249392375422, 0.002383190548876615, 0.002366496432674506, 0.0023498428475610244, 0.002333230049842188, 0.0023166582951962746, 0.0023001278386698956, 0.0022836389346740597, 0.002267191836980273, 0.002250786798716619, 0.0022344240723638654, 0.0022181039097515873, 0.0022018265620542786, 0.002185592279787496, 0.0021694013128040015, 0.002153253910289914, 0.0021371503207608783, 0.0021210907920582323, 0.0021050755713452043, 0.0020891049051031023, 0.0020731790391275177, 0.0020572982185245482, 0.0020414626877070254, 0.0020256726903907485, 0.002009928469590738, 0.0019942302676174903, 0.0019785783260732547, 0.001962972885848314, 0.0019474141871172674, 0.0019319024693353506, 0.0019164379712347372, 0.001901020930820871, 0.0018856515853688009, 0.0018703301714195258, 0.001855056924776362, 0.0018398320805013098, 0.0018246558729114312, 0.001809528535575252, 0.0017944503013091631, 0.0017794214021738374, 0.001764442069470656, 0.0017495125337381524, 0.0017346330247484658, 0.0017198037715037947, 0.0017050250022328855, 0.0016902969443875124, 0.0016756198246389784, 0.0016609938688746273, 0.001646419302194363, 0.0016318963489071936, 0.0016174252325277733, 0.0016030061757729593, 0.0015886394005583933, 0.0015743251279950786, 0.0015600635783859802, 0.0015458549712226357, 0.0015316995251817688, 0.0015175974581219363, 0.0015035489870801682, 0.0014895543282686234, 0.0014756136970712722, 0.0014617273080405736, 0.0014478953748941772, 0.0014341181105116322, 0.0014203957269311078, 0.0014067284353461405, 0.0013931164461023684, 0.0013795599686943093, 0.0013660592117621252, 0.0013526143830884176, 0.0013392256895950285, 0.0013258933373398486, 0.0013126175315136587, 0.0012993984764369648, 0.0012862363755568476, 0.0012731314314438438, 0.001260083845788821, 0.0012470938193998732, 0.0012341615521992348, 0.0012212872432201963, 0.0012084710906040486, 0.0011957132915970327, 0.001183014042547296, 0.001170373538901881, 0.001157791975203711, 0.001145269545088598, 0.0011328064412822621, 0.0011204028555973629, 0.0011080589789305548, 0.001095775001259538, 0.0010835511116401447, 0.001071387498203428, 0.0010592843481527568, 0.0010472418477609487, 0.001035260182367391, 0.0010233395363751963, 0.001011480093248364, 0.0009996820355089478, 0.0009879455447342597, 0.0009762708015540657, 0.0009646579856478102, 0.0009531072757418502, 0.0009416188496067012, 0.000930192884054306, 0.0009188295549353131, 0.0009075290371363649, 0.0008962915045774127, 0.0008851171302090361, 0.0008740060860097829, 0.0008629585429835224, 0.0008519746711568096, 0.0008410546395762749, 0.0008301986163060199, 0.0008194067684250242, 0.0008086792620245891, 0.0007980162622057626, 0.0007874179330768164, 0.0007768844377507041, 0.0007664159383425639]}, {'val_loss': 0.2821979224681854, 'val_acc': 0.9035236835479736, 'train_loss': 0.21694840490818024, 'lrs': [0.0007560125959672177, 0.0007456745707366888, 0.0007354020217577429, 0.0007251951071294379, 0.000715053983940689, 0.0007049788082678546, 0.0006949697351723265, 0.0006850269186981531, 0.0006751505118696647, 0.0006653406666891124, 0.0006555975341343425, 0.0006459212641564564, 0.0006363120056775158, 0.0006267699065882469, 0.0006172951137457573, 0.0006078877729712877, 0.0005985480290479603, 0.0005892760257185481, 0.0005800719056832741, 0.0005709358105975998, 0.0005618678810700567, 0.0005528682566600793, 0.0005439370758758527, 0.0005350744761721868, 0.0005262805939483975, 0.000517555564546208, 0.0005088995222476702, 0.0005003126002730877, 0.0004917949307789772, 0.0004833466448560255, 0.000474967872527077, 0.0004666587427451335, 0.0004584193833913622, 0.000450249921273139, 0.00044215048212208443, 0.00043412119059213895, 0.00042616217025763915, 0.0004182735436114145, 0.00041045543206290856, 0.0004027079559363043, 0.0003950312344686723, 0.0003874253858081431, 0.00037989052701207913, 0.00037242677404527837, 0.00036503424177819076, 0.00035771304398514475, 0.00035046329334260343, 0.0003432851014274225, 0.00033617857871514267, 0.0003291438345782806, 0.00032218097728464924, 0.00031529011399569203, 0.00030847135076483167, 0.00030172479253584106, 0.00029505054314122556, 0.00028844870530062445, 0.00028191938061923415, 0.0002754626695862383, 0.0002690786715732661, 0.0002627674848328625, 0.0002565292064969723, 0.000250363932575449, 0.0002442717579545771, 0.00023825277639560844, 0.00023230708053332356, 0.00022643476187460222, 0.0002206359107970174, 0.00021491061654744343, 0.00020925896724068304, 0.00020368104985810952, 0.00019817695024633073, 0.0001927467531158667, 0.00018739054203984463, 0.00018210839945271377, 0.00017690040664897704, 0.00017176664378194036, 0.00016670718986247637, 0.0001617221227578121, 0.0001568115191903267, 0.00015197545473637496, 0.0001472140038251201, 0.00014252723973739022, 0.00013791523460455183, 0.00013337805940739594, 0.00012891578397504936, 0.00012452847698389842, 0.00012021620595653088, 0.00011597903726070014, 0.00011181703610829936, 0.0001077302665543606, 0.00010371879149607011, 9.978267267179568e-05, 9.592197066014252e-05, 9.213674487901611e-05, 8.842705358471026e-05, 8.479295387100891e-05, 8.123450166830845e-05, 7.775175174275681e-05, 7.434475769541134e-05, 7.101357196141227e-05, 6.775824580917608e-05, 6.457882933960675e-05, 6.147537148532632e-05, 5.844792000991892e-05, 5.549652150719739e-05, 5.262122140048609e-05, 4.982206394192038e-05, 4.7099092211767726e-05, 4.445234811776318e-05, 4.1881872394466076e-05, 3.9387704602630494e-05, 3.696988312859798e-05, 3.462844518370691e-05, 3.236342680371907e-05, 3.0174862848266757e-05, 2.8062787000313215e-05, 2.602723176563694e-05, 2.4068228472330414e-05, 2.2185807270318283e-05, 2.037999713089215e-05, 1.865082584626651e-05, 1.6998320029149647e-05, 1.5422505112333944e-05, 1.3923405348305672e-05, 1.2501043808871933e-05, 1.115544238480318e-05, 9.886621785499035e-06, 8.694601538666887e-06, 7.579399990024905e-06, 6.541034303015605e-06, 5.579520458545498e-06, 4.694873254735849e-06, 3.88710630669675e-06, 3.1562320463189498e-06, 2.502261722078458e-06, 1.9252053988672356e-06, 1.4250719578366548e-06, 1.0018690962603873e-06, 6.556033274167199e-07, 3.8627998048974616e-07, 1.93903200483324e-07, 7.847594816112395e-08, 4e-08]}]\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Optimizer\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import math\n",
        "# import warnings\n",
        "\n",
        "\n",
        "class OneCycleLR(StepLR):\n",
        "    def __init__(self,\n",
        "                 optimizer,\n",
        "                 max_lr,\n",
        "                 total_steps=None,\n",
        "                 epochs=None,\n",
        "                 steps_per_epoch=None,\n",
        "                 pct_start=0.3,\n",
        "                 anneal_strategy='cos',\n",
        "                 cycle_momentum=True,\n",
        "                 base_momentum=0.85,\n",
        "                 max_momentum=0.95,\n",
        "                 div_factor=25.,\n",
        "                 final_div_factor=1e4,\n",
        "                 three_phase=False,\n",
        "                 last_epoch=-1,\n",
        "                 verbose=False):\n",
        "\n",
        "        self.last_epoch = last_epoch\n",
        "\n",
        "        # Validate optimizer\n",
        "        if not isinstance(optimizer, Optimizer):\n",
        "            raise TypeError('{} is not an Optimizer'.format(\n",
        "                type(optimizer).__name__))\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        # Validate total_steps\n",
        "        if total_steps is None and epochs is None and steps_per_epoch is None:\n",
        "            raise ValueError(\"You must define either total_steps OR (epochs AND steps_per_epoch)\")\n",
        "        elif total_steps is not None:\n",
        "            if total_steps <= 0 or not isinstance(total_steps, int):\n",
        "                raise ValueError(\"Expected positive integer total_steps, but got {}\".format(total_steps))\n",
        "            self.total_steps = total_steps\n",
        "        else:\n",
        "            if epochs <= 0 or not isinstance(epochs, int):\n",
        "                raise ValueError(\"Expected positive integer epochs, but got {}\".format(epochs))\n",
        "            if steps_per_epoch <= 0 or not isinstance(steps_per_epoch, int):\n",
        "                raise ValueError(\"Expected positive integer steps_per_epoch, but got {}\".format(steps_per_epoch))\n",
        "            self.total_steps = epochs * steps_per_epoch\n",
        "\n",
        "        if three_phase:\n",
        "            self._schedule_phases = [\n",
        "                {\n",
        "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'max_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'base_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': float(2 * pct_start * self.total_steps) - 2,\n",
        "                    'start_lr': 'max_lr',\n",
        "                    'end_lr': 'initial_lr',\n",
        "                    'start_momentum': 'base_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': self.total_steps - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'min_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "            ]\n",
        "        else:\n",
        "            self._schedule_phases = [\n",
        "                {\n",
        "                    'end_step': float(pct_start * self.total_steps) - 1,\n",
        "                    'start_lr': 'initial_lr',\n",
        "                    'end_lr': 'max_lr',\n",
        "                    'start_momentum': 'max_momentum',\n",
        "                    'end_momentum': 'base_momentum',\n",
        "                },\n",
        "                {\n",
        "                    'end_step': self.total_steps - 1,\n",
        "                    'start_lr': 'max_lr',\n",
        "                    'end_lr': 'min_lr',\n",
        "                    'start_momentum': 'base_momentum',\n",
        "                    'end_momentum': 'max_momentum',\n",
        "                },\n",
        "            ]\n",
        "\n",
        "        # Validate pct_start\n",
        "        if pct_start < 0 or pct_start > 1 or not isinstance(pct_start, float):\n",
        "            raise ValueError(\"Expected float between 0 and 1 pct_start, but got {}\".format(pct_start))\n",
        "\n",
        "        # Validate anneal_strategy\n",
        "        if anneal_strategy not in ['cos', 'linear']:\n",
        "            raise ValueError(\"anneal_strategy must by one of 'cos' or 'linear', instead got {}\".format(anneal_strategy))\n",
        "        elif anneal_strategy == 'cos':\n",
        "            self.anneal_func = self._annealing_cos\n",
        "        elif anneal_strategy == 'linear':\n",
        "            self.anneal_func = self._annealing_linear\n",
        "\n",
        "        # Initialize learning rate variables\n",
        "        max_lrs = self._format_param('max_lr', self.optimizer, max_lr)\n",
        "        if last_epoch == -1:\n",
        "            for idx, group in enumerate(self.optimizer.param_groups):\n",
        "                group['initial_lr'] = max_lrs[idx] / div_factor\n",
        "                group['max_lr'] = max_lrs[idx]\n",
        "                group['min_lr'] = group['initial_lr'] / final_div_factor\n",
        "\n",
        "        # Initialize momentum variables\n",
        "        self.cycle_momentum = cycle_momentum\n",
        "        if self.cycle_momentum:\n",
        "            if 'momentum' not in self.optimizer.defaults and 'betas' not in self.optimizer.defaults:\n",
        "                raise ValueError('optimizer must support momentum with `cycle_momentum` option enabled')\n",
        "            self.use_beta1 = 'betas' in self.optimizer.defaults\n",
        "            max_momentums = self._format_param('max_momentum', optimizer, max_momentum)\n",
        "            base_momentums = self._format_param('base_momentum', optimizer, base_momentum)\n",
        "            if last_epoch == -1:\n",
        "                for m_momentum, b_momentum, group in zip(max_momentums, base_momentums, optimizer.param_groups):\n",
        "                    if self.use_beta1:\n",
        "                        _, beta2 = group['betas']\n",
        "                        group['betas'] = (m_momentum, beta2)\n",
        "                    else:\n",
        "                        group['momentum'] = m_momentum\n",
        "                    group['max_momentum'] = m_momentum\n",
        "                    group['base_momentum'] = b_momentum\n",
        "\n",
        "        super(OneCycleLR, self).__init__(optimizer, last_epoch, verbose)\n",
        "\n",
        "    def _format_param(self, name, optimizer, param):\n",
        "        \"\"\"Return correctly formatted lr/momentum for each param group.\"\"\"\n",
        "        if isinstance(param, (list, tuple)):\n",
        "            if len(param) != len(optimizer.param_groups):\n",
        "                raise ValueError(\"expected {} values for {}, got {}\".format(\n",
        "                    len(optimizer.param_groups), name, len(param)))\n",
        "            return param\n",
        "        else:\n",
        "            return [param] * len(optimizer.param_groups)\n",
        "\n",
        "    def _annealing_cos(self, start, end, pct):\n",
        "        \"Cosine anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
        "        cos_out = math.cos(math.pi * pct) + 1\n",
        "        return end + (start - end) / 2.0 * cos_out\n",
        "\n",
        "    def _annealing_linear(self, start, end, pct):\n",
        "        \"Linearly anneal from `start` to `end` as pct goes from 0.0 to 1.0.\"\n",
        "        return (end - start) * pct + start\n",
        "\n",
        "    def get_lr(self):\n",
        "        pass\n",
        "        # if not self._get_lr_called_within_step:\n",
        "          #  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
        "           #               \"please use `get_last_lr()`.\", UserWarning)\n",
        "\n",
        "        lrs = []\n",
        "        step_num = self.last_epoch\n",
        "\n",
        "        if step_num > self.total_steps:\n",
        "            raise ValueError(\"Tried to step {} times. The specified number of total steps is {}\"\n",
        "                             .format(step_num + 1, self.total_steps))\n",
        "\n",
        "        for group in self.optimizer.param_groups:\n",
        "            start_step = 0\n",
        "            for i, phase in enumerate(self._schedule_phases):\n",
        "                end_step = phase['end_step']\n",
        "                if step_num <= end_step or i == len(self._schedule_phases) - 1:\n",
        "                    pct = (step_num - start_step) / (end_step - start_step)\n",
        "                    computed_lr = self.anneal_func(group[phase['start_lr']], group[phase['end_lr']], pct)\n",
        "                    if self.cycle_momentum:\n",
        "                        computed_momentum = self.anneal_func(group[phase['start_momentum']], group[phase['end_momentum']], pct)\n",
        "                    break\n",
        "                start_step = phase['end_step']\n",
        "\n",
        "            lrs.append(computed_lr)\n",
        "            if self.cycle_momentum:\n",
        "                if self.use_beta1:\n",
        "                    _, beta2 = group['betas']\n",
        "                    group['betas'] = (computed_momentum, beta2)\n",
        "                else:\n",
        "                    group['momentum'] = computed_momentum\n",
        "\n",
        "        return lrs\n",
        "\n",
        "\n",
        "import torch\n",
        "import tarfile\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as tt\n",
        "import torch.optim.lr_scheduler\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "dataset_url = \"https://s3.amazonaws.com/fast-ai-imageclas/cifar10.tgz\"\n",
        "download_url(dataset_url, '.')\n",
        "\n",
        "with tarfile.open('./cifar10.tgz', 'r:gz') as tar:\n",
        "    tar.extractall(path='./data')\n",
        "\n",
        "data_dir = './data/cifar10'\n",
        "\n",
        "stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "train_tfms = tt.Compose([tt.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "                         tt.RandomHorizontalFlip(),\n",
        "                         tt.ToTensor(),\n",
        "                         tt.Normalize(*stats, inplace=True)])\n",
        "valid_tfms = tt.Compose([tt.ToTensor(), tt.Normalize(*stats)])\n",
        "\n",
        "train_ds = ImageFolder(data_dir+'/train', train_tfms)\n",
        "valid_ds = ImageFolder(data_dir+'/test', valid_tfms)\n",
        "\n",
        "batch_size = 350\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)\n",
        "\n",
        "\n",
        "def get_default_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "class DeviceDataLoader:\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "device = get_default_device()\n",
        "\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "valid_dl = DeviceDataLoader(valid_dl, device)\n",
        "\n",
        "\n",
        "class SimpleResidualBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.conv2(out)\n",
        "        return self.relu2(out) + x\n",
        "\n",
        "\n",
        "class ImgClassificationBase(nn.Module):\n",
        "    def accuracy(self, output, labels):\n",
        "        _, preds = torch.max(output, dim=1)\n",
        "        return torch.tensor(torch.sum(torch.eq(preds, labels)).item() / len(preds))\n",
        "\n",
        "    def evaluate(self, model, val_loader):\n",
        "        output = [model.validation_step(batch) for batch in val_loader]\n",
        "        return model.validation_epoch_end(output)\n",
        "\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)  # Calculate loss\n",
        "        acc = self.accuracy(out, labels)  # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()  # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()  # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ResNet9(ImgClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.MaxPool2d(4),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "model = to_device(ResNet9(3, 10), device)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader,\n",
        "                  weight_decay=0.0, grad_clip=None, opt_func=torch.optim.Adam):\n",
        "    torch.cuda.empty_cache()\n",
        "    history = []\n",
        "\n",
        "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
        "    sched = OneCycleLR(optimizer, max_lr, epochs=epochs, steps_per_epoch=len(train_loader))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        lrs = []\n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "\n",
        "            if grad_clip:\n",
        "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            lrs.append(get_lr(optimizer))\n",
        "            sched.step()\n",
        "\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        result['lrs'] = lrs\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history\n",
        "  \n",
        "def saveparams(model):\n",
        "  path = input(\"How would you like to name this save?\")\n",
        "  torch.save(model, f\"{path}.pth\")\n",
        "\n",
        "def predict_img(model, img):\n",
        "  xb = to_device(img.unsqueeze(0), device)\n",
        "  yb = model(xb)\n",
        "  _, preds = torch.max(yb, dim=1)\n",
        "  return train_ds.classes[preds[0].items()]\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    epochs = 8\n",
        "    max_lr = 0.01\n",
        "    grad_clip = 0.1\n",
        "    weight_decay = 1e-4\n",
        "    opt_func = torch.optim.Adam\n",
        "    history = fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, grad_clip=grad_clip, weight_decay=weight_decay, opt_func=opt_func)\n",
        "    print(history)"
      ]
    }
  ]
}